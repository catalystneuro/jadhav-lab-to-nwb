"""Primary class for converting experiment-specific behavior."""
from pynwb.file import NWBFile
from pydantic import FilePath, DirectoryPath
import numpy as np
import re
from pathlib import Path

from neuroconv.basedatainterface import BaseDataInterface
from neuroconv.tools import nwb_helpers
from neuroconv.utils import get_base_schema
from ndx_events import Events


class Olson2024BehaviorInterface(BaseDataInterface):
    """Behavior interface for olson_2024 conversion"""

    keywords = ("behavior",)

    def __init__(self, folder_path: DirectoryPath):
        super().__init__(folder_path=folder_path)

    def get_metadata_schema(self):
        metadata_schema = super().get_metadata_schema()
        metadata_schema["properties"]["Behavior"] = get_base_schema(tag="Behavior")
        metadata_schema["properties"]["Behavior"]["properties"]["Module"] = {
            "properties": {
                "name": {"type": "string"},
                "description": {"type": "string"},
            },
        }
        metadata_schema["properties"]["Behavior"]["properties"]["Events"] = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "id": {"type": "string"},
                    "name": {"type": "string"},
                    "description": {"type": "string"},
                },
            },
        }
        return metadata_schema

    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: dict):
        folder_path = Path(self.source_data["folder_path"])
        behavior_module = nwb_helpers.get_module(
            nwbfile=nwbfile,
            name=metadata["Behavior"]["Module"]["name"],
            description=metadata["Behavior"]["Module"]["description"],
        )
        for file_path in folder_path.glob(r"*.dat"):
            fieldsText = readTrodesExtractedDataFile(file_path)
            rate = np.asarray(fieldsText["clockrate"], dtype="float64")
            timestamps = fieldsText["data"]["time"][fieldsText["data"]["state"] == 1]
            timestamps = np.asarray(timestamps, dtype="float64") / rate
            event_id = fieldsText["id"]
            event_metadata = next(
                event_metadata for event_metadata in metadata["Behavior"]["Events"] if event_metadata["id"] == event_id
            )
            event = Events(
                name=event_metadata["name"],
                description=event_metadata["description"],
                timestamps=timestamps,
            )
            behavior_module.add(event)


def readTrodesExtractedDataFile(filename: FilePath) -> dict:
    """Read Trodes Extracted Data File (.dat) and return as a dictionary.

    Adapted from https://docs.spikegadgets.com/en/latest/basic/ExportFunctions.html

    Parameters
    ----------
    filename : FilePath
        Path to the .dat file to read.

    Returns
    -------
    dict
        The contents of the .dat file as a dictionary
    """
    with open(filename, "rb") as f:
        # Check if first line is start of settings block
        if f.readline().decode("ascii").strip() != "<Start settings>":
            raise Exception("Settings format not supported")
        fields = True
        fieldsText = {}
        for line in f:
            # Read through block of settings
            if fields:
                line = line.decode("ascii").strip()
                # filling in fields dict
                if line != "<End settings>":
                    vals = line.split(": ")
                    fieldsText.update({vals[0].lower(): vals[1]})
                # End of settings block, signal end of fields
                else:
                    fields = False
                    dt = parseFields(fieldsText["fields"])
                    fieldsText["data"] = np.zeros([1], dtype=dt)
                    break
        # Reads rest of file at once, using dtype format generated by parseFields()
        dt = parseFields(fieldsText["fields"])
        data = np.fromfile(f, dt)
        fieldsText.update({"data": data})
        return fieldsText


def parseFields(fieldstr: str) -> np.dtype:
    """Parse the fields string from a Trodes Extracted Data File and return as a numpy dtype.

    Adapted from https://docs.spikegadgets.com/en/latest/basic/ExportFunctions.html

    Parameters
    ----------
    fieldstr : str
        The fields string from a Trodes Extracted Data File.

    Returns
    -------
    np.dtype
        The fields string as a numpy dtype.
    """
    # Returns np.dtype from field string
    sep = re.split("\s", re.sub(r"\>\<|\>|\<", " ", fieldstr).strip())
    # print(sep)
    typearr = []
    # Every two elmts is fieldname followed by datatype
    for i in range(0, sep.__len__(), 2):
        fieldname = sep[i]
        repeats = 1
        ftype = "uint32"
        # Finds if a <num>* is included in datatype
        if sep[i + 1].__contains__("*"):
            temptypes = re.split("\*", sep[i + 1])
            # Results in the correct assignment, whether str is num*dtype or dtype*num
            ftype = temptypes[temptypes[0].isdigit()]
            repeats = int(temptypes[temptypes[1].isdigit()])
        else:
            ftype = sep[i + 1]
        try:
            fieldtype = getattr(np, ftype)
        except AttributeError:
            print(ftype + " is not a valid field type.\n")
            exit(1)
        else:
            typearr.append((str(fieldname), fieldtype, repeats))
    return np.dtype(typearr)
